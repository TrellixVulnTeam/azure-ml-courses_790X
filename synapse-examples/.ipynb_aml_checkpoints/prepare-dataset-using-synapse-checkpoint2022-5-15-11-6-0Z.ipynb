{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1654778100432
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace, LinkedService\n",
        "from azureml.widgets import RunDetails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1654778100731
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "synapse_linked = 'synw-dmpbackup-westeu-01p-linked'\n",
        "synapse_compute_name = 'cc-small'\n",
        "synapse_pool_name = 'test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1654778101482
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ws = Workspace.from_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Retrieve the link between your Azure Synapse Analytics workspace and your Azure Machine Learning workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1654778102543
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Service: LinkedService(workspace=Workspace.create(name='mlw-dmpbackup-westeu-01p', subscription_id='25a89471-60b3-4c91-b44f-ca49f38e6137', resource_group='rg-dmpbackup-westeu-01p'), name=synw-dmpbackup-westeu-01p-linked, type=LinkedServiceLinkType.synapse, linked_service_resource_id=/subscriptions/25a89471-60b3-4c91-b44f-ca49f38e6137/resourceGroups/rg-dmpbackup-westeu-01p/providers/Microsoft.Synapse/workspaces/synw-dmpbackup-westeu-01p, system_assigned_identity_principal_id=4920cd8e-47e7-4407-8d4b-2dc91a133c2f\n"
          ]
        }
      ],
      "source": [
        "for service in LinkedService.list(ws) : \n",
        "    print(f\"Service: {service}\")\n",
        "\n",
        "# Retrieve a known linked service\n",
        "linked_service = LinkedService.get(ws, synapse_linked)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Attach your Apache spark pool as a compute target for Azure Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1654778103761
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provisioning operation finished, operation \"Succeeded\"\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.compute import SynapseCompute, ComputeTarget\n",
        "\n",
        "\n",
        "attach_config = SynapseCompute.attach_configuration(\n",
        "        linked_service=linked_service,\n",
        "        type=\"SynapseSpark\",\n",
        "        pool_name=synapse_pool_name,\n",
        ")\n",
        "\n",
        "synapse_compute = ComputeTarget.attach(\n",
        "        workspace=ws,\n",
        "        name=synapse_compute_name,\n",
        "        attach_configuration=attach_config,\n",
        ")\n",
        "\n",
        "synapse_compute.wait_for_completion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create a SynapseSparkStep that uses the linked Apache Spark pool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1654778398839
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "only conda_dependencies specified in environment will be used in Synapse Spark run.\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.environment import Environment\n",
        "from azureml.pipeline.steps import SynapseSparkStep\n",
        "\n",
        "env = Environment(name=\"myenv\")\n",
        "env.python.conda_dependencies.add_pip_package(\"azureml-core>=1.20.0\")\n",
        "\n",
        "step_1 = SynapseSparkStep(\n",
        "    name='synapse-spark',\n",
        "    file='prep-dataset-synapse.py',\n",
        "    source_directory=\"./code\", \n",
        "    arguments=[\"--hday\", '2021-12-01', '--out_dataset_name', 'one-user-dataset', '--out_dataset_desc', 'dataset with only one user'],\n",
        "    compute_target=synapse_compute_name,\n",
        "    driver_memory=\"7g\",\n",
        "    driver_cores=4,\n",
        "    executor_memory=\"7g\",\n",
        "    executor_cores=2,\n",
        "    num_executors=1,\n",
        "    environment=env,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.pipeline.core import Pipeline\n",
        "\n",
        "pipeline = Pipeline(workspace=ws, steps=[step_1])\n",
        "pipeline_run = pipeline.submit('synapse-pipeline', regenerate_outputs=True)\n",
        "RunDetails(pipeline_run).show()\n",
        "pipeline_run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import RunConfiguration\n",
        "from azureml.core import ScriptRunConfig \n",
        "\n",
        "from azureml.core.environment import CondaDependencies\n",
        "conda_dep = CondaDependencies()\n",
        "conda_dep.add_pip_package(\"azureml-core==1.20.0\")\n",
        "\n",
        "run_config = RunConfiguration(framework=\"pyspark\")\n",
        "run_config.target = synapse_compute_name\n",
        "\n",
        "run_config.spark.configuration[\"spark.driver.memory\"] = \"7g\" \n",
        "run_config.spark.configuration[\"spark.driver.cores\"] = 2 \n",
        "run_config.spark.configuration[\"spark.executor.memory\"] = \"7g\" \n",
        "run_config.spark.configuration[\"spark.executor.cores\"] = 1 \n",
        "run_config.spark.configuration[\"spark.executor.instances\"] = 1 \n",
        "\n",
        "run_config.environment.python.conda_dependencies = conda_dep\n",
        "\n",
        "script_run_config=ScriptRunConfig(\n",
        "    source_directory='./code',\n",
        "    script='prep-dataset-synapse.py',\n",
        "    arguments=[\"--hday\", '2021-12-01', '--out_dataset_name', 'one-user-dataset', '--out_dataset_desc', 'dataset with only one user'],\n",
        "    run_config=run_config,\n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed9b10e1abd54cedad20b26e86fcaee9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/synapse-spark_1654782772_9f61aa5e?wsid=/subscriptions/25a89471-60b3-4c91-b44f-ca49f38e6137/resourcegroups/rg-dmpbackup-westeu-01p/workspaces/mlw-dmpbackup-westeu-01p&tid=041d21aa-b4ab-4ad1-891d-62207b3367ef\", \"run_id\": \"synapse-spark_1654782772_9f61aa5e\", \"run_properties\": {\"run_id\": \"synapse-spark_1654782772_9f61aa5e\", \"created_utc\": \"2022-06-09T13:52:52.998188Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"synapsespark\", \"ContentSnapshotId\": \"8d97b488-2217-4ae8-8fa4-b9f4e7dab5a1\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2022-06-09T13:59:43.940907Z\", \"status\": \"Failed\", \"log_files\": {\"logs/azureml/driver/stderr\": \"https://stdmpbackupmlwwesteu01p.blob.core.windows.net/azureml/ExperimentRun/dcid.synapse-spark_1654782772_9f61aa5e/logs/azureml/driver/stderr?sv=2019-07-07&sr=b&sig=Kqk0N8hrve4Nd7wCMMxNrBZc7w058GcMwiKNFgHGEz4%3D&skoid=c1efd837-baab-4c88-a36e-3f214eaff049&sktid=041d21aa-b4ab-4ad1-891d-62207b3367ef&skt=2022-06-09T12%3A13%3A53Z&ske=2022-06-10T20%3A23%3A53Z&sks=b&skv=2019-07-07&st=2022-06-09T13%3A48%3A17Z&se=2022-06-09T21%3A58%3A17Z&sp=r\", \"logs/azureml/driver/stdout\": \"https://stdmpbackupmlwwesteu01p.blob.core.windows.net/azureml/ExperimentRun/dcid.synapse-spark_1654782772_9f61aa5e/logs/azureml/driver/stdout?sv=2019-07-07&sr=b&sig=WMCDh3ylGK9n6J0P26IsgXPJj9%2BWsjkKX2TccSr%2B8Uc%3D&skoid=c1efd837-baab-4c88-a36e-3f214eaff049&sktid=041d21aa-b4ab-4ad1-891d-62207b3367ef&skt=2022-06-09T12%3A13%3A53Z&ske=2022-06-10T20%3A23%3A53Z&sks=b&skv=2019-07-07&st=2022-06-09T13%3A48%3A17Z&se=2022-06-09T21%3A58%3A17Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/driver/stderr\", \"logs/azureml/driver/stdout\"]], \"run_duration\": \"0:06:50\", \"run_number\": \"1654782773\", \"run_queued_details\": {\"status\": \"Failed\", \"details\": null}}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"SLF4J: Class path contains multiple SLF4J bindings.\\nSLF4J: Found binding in [jar:file:/usr/hdp/5.0-62565507/spark3/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\nSLF4J: Found binding in [jar:file:/usr/hdp/5.0-62565507/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\\n22/06/09 13:57:51 INFO SignalUtils: Registering signal handler for TERM\\n22/06/09 13:57:51 ERROR RawSocketSender: org.fluentd.logger.sender.RawSocketSender\\njava.net.SocketTimeoutException\\n\\tat java.net.SocksSocketImpl.remainingMillis(SocksSocketImpl.java:111)\\n\\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\\n\\tat java.net.Socket.connect(Socket.java:607)\\n\\tat org.fluentd.logger.sender.RawSocketSender.connect(RawSocketSender.java:85)\\n\\tat org.fluentd.logger.sender.RawSocketSender.reconnect(RawSocketSender.java:94)\\n\\tat org.fluentd.logger.sender.RawSocketSender.flush(RawSocketSender.java:193)\\n\\tat org.fluentd.logger.sender.RawSocketSender.send(RawSocketSender.java:184)\\n\\tat org.fluentd.logger.sender.RawSocketSender.emit(RawSocketSender.java:149)\\n\\tat org.fluentd.logger.sender.RawSocketSender.emit(RawSocketSender.java:131)\\n\\tat org.fluentd.logger.sender.RawSocketSender.emit(RawSocketSender.java:126)\\n\\tat org.fluentd.logger.FluentLogger.log(FluentLogger.java:101)\\n\\tat org.fluentd.logger.FluentLogger.log(FluentLogger.java:86)\\n\\tat com.microsoft.mdsdclient.MessageSendingRunnable$1.call(Unknown Source)\\n\\tat com.microsoft.mdsdclient.MessageSendingRunnable$1.call(Unknown Source)\\n\\tat com.microsoft.mdsdclient.RetryUtil.retry(Unknown Source)\\n\\tat com.microsoft.mdsdclient.RetryUtil.retry(Unknown Source)\\n\\tat com.microsoft.mdsdclient.MessageSendingRunnable.sendMessage(Unknown Source)\\n\\tat com.microsoft.mdsdclient.MessageSendingRunnable.run(Unknown Source)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n22/06/09 13:57:51 INFO SignalUtils: Registering signal handler for HUP\\n22/06/09 13:57:51 INFO SignalUtils: Registering signal handler for INT\\n22/06/09 13:57:52 INFO SecurityManager: Changing view acls to: trusted-service-user\\n22/06/09 13:57:52 INFO SecurityManager: Changing modify acls to: trusted-service-user\\n22/06/09 13:57:52 INFO SecurityManager: Changing view acls groups to: \\n22/06/09 13:57:52 INFO SecurityManager: Changing modify acls groups to: \\n22/06/09 13:57:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(trusted-service-user); groups with view permissions: Set(); users  with modify permissions: Set(trusted-service-user); groups with modify permissions: Set()\\n22/06/09 13:57:52 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1654782878474_0001_000001\\n22/06/09 13:57:52 INFO ApplicationMaster: Starting the user application in a separate Thread\\n22/06/09 13:57:52 INFO ApplicationMaster: Waiting for spark context initialization...\\n22/06/09 13:57:52 INFO PythonRunner$: Initialized PythonRunnerOutputStream plugin org.apache.spark.microsoft.tools.api.plugin.MSToolsPythonRunnerOutputStreamPlugin.\\n22/06/09 13:57:52 INFO MetricsConfig: Loaded properties from hadoop-metrics2.properties\\n22/06/09 13:57:52 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\\n22/06/09 13:57:52 INFO MetricsSystemImpl: azure-file-system metrics system started\\n22/06/09 13:57:59 INFO SparkContext: Running Spark version 3.1.2.5.0-62565507\\n22/06/09 13:57:59 INFO ResourceUtils: ==============================================================\\n22/06/09 13:57:59 INFO ResourceUtils: No custom resources configured for spark.driver.\\n22/06/09 13:57:59 INFO ResourceUtils: ==============================================================\\n22/06/09 13:57:59 INFO SparkContext: Submitted application: Azure ML Experiment\\n22/06/09 13:57:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 384, script: , vendor: , cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 7168, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\\n22/06/09 13:57:59 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\\n22/06/09 13:57:59 INFO ResourceProfileManager: Added ResourceProfile id: 0\\n22/06/09 13:57:59 INFO SecurityManager: Changing view acls to: trusted-service-user\\n22/06/09 13:57:59 INFO SecurityManager: Changing modify acls to: trusted-service-user\\n22/06/09 13:57:59 INFO SecurityManager: Changing view acls groups to: \\n22/06/09 13:57:59 INFO SecurityManager: Changing modify acls groups to: \\n22/06/09 13:57:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(trusted-service-user); groups with view permissions: Set(); users  with modify permissions: Set(trusted-service-user); groups with modify permissions: Set()\\n22/06/09 13:58:00 INFO Utils: Successfully started service 'sparkDriver' on port 43691.\\n22/06/09 13:58:00 INFO SparkEnv: Registering MapOutputTracker\\n22/06/09 13:58:00 INFO SparkEnv: Registering BlockManagerMaster\\n22/06/09 13:58:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\\n22/06/09 13:58:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\\n22/06/09 13:58:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\\n22/06/09 13:58:00 INFO DiskBlockManager: Created local directory at /mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1654782878474_0001/blockmgr-653481e1-e758-447f-bc77-e626b11f55e8\\n22/06/09 13:58:00 INFO MemoryStore: MemoryStore started with capacity 3.6 GiB\\n22/06/09 13:58:00 INFO SparkEnv: Registering OutputCommitCoordinator\\n22/06/09 13:58:01 INFO log: Logging initialized @11236ms to org.sparkproject.jetty.util.log.Slf4jLog\\n22/06/09 13:58:01 INFO Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_312-8u312-b07-0ubuntu1~18.04-b07\\n22/06/09 13:58:01 INFO Server: Started @11457ms\\n22/06/09 13:58:01 INFO AbstractConnector: Started ServerConnector@8b78470{HTTP/1.1, (http/1.1)}{0.0.0.0:38603}\\n22/06/09 13:58:01 INFO Utils: Successfully started service 'SparkUI' on port 38603.\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@12e927ae{/jobs,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3121500b{/jobs/json,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4bfd6d2c{/jobs/job,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2c793eda{/jobs/job/json,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@557823d0{/stages,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@a43b3a7{/stages/json,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@253418e6{/stages/stage,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5fbbda4c{/stages/stage/json,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6567ba5e{/stages/pool,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2a6c403b{/stages/pool/json,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@30aab65c{/storage,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@240bf6fa{/storage/json,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@51f500c3{/storage/rdd,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3d696759{/storage/rdd/json,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@395f71c1{/environment,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@18398eae{/environment/json,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@449badcc{/executors,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5d058f00{/executors/json,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5eeec711{/executors/threadDump,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@22af520{/executors/threadDump/json,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@a9ef462{/static,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@11087ece{/,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@71f60ca8{/api,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /metrics: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7615c244{/metrics,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4b679e84{/jobs/job/kill,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4c9df37c{/stages/stage/kill,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://vm-30847173:38603\\n22/06/09 13:58:01 INFO YarnClusterScheduler: Created YarnClusterScheduler\\n22/06/09 13:58:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39447.\\n22/06/09 13:58:01 INFO NettyBlockTransferService: Server created on vm-30847173:39447\\n22/06/09 13:58:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\\n22/06/09 13:58:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, vm-30847173, 39447, None)\\n22/06/09 13:58:01 INFO BlockManagerMasterEndpoint: Registering block manager vm-30847173:39447 with 3.6 GiB RAM, BlockManagerId(driver, vm-30847173, 39447, None)\\n22/06/09 13:58:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, vm-30847173, 39447, None)\\n22/06/09 13:58:01 INFO BlockManager: external shuffle service port = 7337\\n22/06/09 13:58:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, vm-30847173, 39447, None)\\n22/06/09 13:58:01 INFO SparkObservabilityBus: SparkDiagnosticEmitter: ShoeboxEmitter initialized\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@179ad05a{/metrics/json,null,AVAILABLE,@Spark}\\n22/06/09 13:58:01 INFO ServerInfo: Adding filter to /metrics/prometheus: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:01 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5747f097{/metrics/prometheus,null,AVAILABLE,@Spark}\\n22/06/09 13:58:02 INFO SingleEventLogFileWriter: Logging events to wasbs://6ebbb482-091e-4d4f-8d36-112ba23300db@dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net/events/85/eventLogs/application_1654782878474_0001_1.inprogress\\n22/06/09 13:58:02 INFO EnhancementLiveStatusPlugin: Enhancement Live App Status Plugin initialization\\n22/06/09 13:58:02 INFO EnhancementLiveStatusPlugin: Live app enhancement was enabled\\n22/06/09 13:58:02 INFO EnhancementAppStatusListener: attach Enhancement AppStatus Listener on live application\\n22/06/09 13:58:02 INFO DataOperations: configuration: spark.data.maxRecords: 1000\\n22/06/09 13:58:03 INFO SparkContext: Registered live app status plugin org.apache.spark.ui.EnhancementLiveStatusPlugin\\n22/06/09 13:58:03 INFO SparkContext: Registered live app status plugin org.apache.spark.diagnostic.synapse.SparkDiagnosticPlugin\\n22/06/09 13:58:03 INFO RpcAppSparkContextServer: Opening remote SparkContext service at 10.25.96.7:18083, remoteSparkContext/remoteSparkContextEndpoint\\n22/06/09 13:58:03 INFO RemoteSparkContextServer: Opening remote SparkContext server\\n22/06/09 13:58:03 INFO SecurityManager: Changing view acls to: trusted-service-user\\n22/06/09 13:58:03 INFO SecurityManager: Changing modify acls to: trusted-service-user\\n22/06/09 13:58:03 INFO SecurityManager: Changing view acls groups to: \\n22/06/09 13:58:03 INFO SecurityManager: Changing modify acls groups to: \\n22/06/09 13:58:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(trusted-service-user); groups with view permissions: Set(); users  with modify permissions: Set(trusted-service-user); groups with modify permissions: Set()\\n22/06/09 13:58:03 INFO Utils: Successfully started service 'remoteSparkContext' on port 18083.\\n22/06/09 13:58:03 INFO RemoteSparkContextServer: Will serve remote SparkContext on 10.25.96.7:18083, remoteSparkContext/remoteSparkContextEndpoint\\n22/06/09 13:58:04 INFO RpcAppListener: Got host of RPC history server from node info: vm-30847173\\n22/06/09 13:58:04 INFO RpcAppSender: Opening RPC app sender\\n22/06/09 13:58:04 INFO SecurityManager: Changing view acls to: trusted-service-user\\n22/06/09 13:58:04 INFO SecurityManager: Changing modify acls to: trusted-service-user\\n22/06/09 13:58:04 INFO SecurityManager: Changing view acls groups to: \\n22/06/09 13:58:04 INFO SecurityManager: Changing modify acls groups to: \\n22/06/09 13:58:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(trusted-service-user); groups with view permissions: Set(); users  with modify permissions: Set(trusted-service-user); groups with modify permissions: Set()\\n22/06/09 13:58:04 INFO TransportClientFactory: Successfully created connection to vm-30847173/10.25.96.7:18082 after 47 ms (0 ms spent in bootstraps)\\n22/06/09 13:58:04 INFO RpcAppSender: Will send events to RPC history server at vm-30847173:18082, rpcHistoryServer/rpcHistoryServerEndpoint\\n22/06/09 13:58:04 INFO SparkContext: Registered live app status plugin org.apache.spark.deploy.history.rpc.app.RpcAppLivePlugin\\n22/06/09 13:58:04 INFO EventsWriter$: Using the default value false for spark.peregrine.log.disableAnon\\n22/06/09 13:58:04 INFO EventsWriter$: Using the default value false for spark.peregrine.log.prodOverride\\n22/06/09 13:58:04 INFO SparkContext: Registered listener com.microsoft.hdinsight.spark.metrics.SparkMetricsListener\\n22/06/09 13:58:04 INFO SparkContext: Registered listener com.microsoft.peregrine.spark.listeners.PeregrineListenerSynapse\\n22/06/09 13:58:04 INFO SparkContext: Registered listener org.apache.spark.listeners.LogAnalyticsSparkListener\\n22/06/09 13:58:04 INFO SparkContext: Registered listener com.microsoft.impulse.analyze.eventLog.ImpulseListener\\n22/06/09 13:58:04 INFO RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]\\n22/06/09 13:58:04 INFO YarnRMClient: Registering the ApplicationMaster\\n22/06/09 13:58:04 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...\\n22/06/09 13:58:05 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm1]\\n22/06/09 13:58:05 INFO ApplicationMaster: Preparing Local resources\\n22/06/09 13:58:05 INFO ApplicationMaster: \\n===============================================================================\\nDefault YARN executor launch context:\\n  env:\\n    CLASSPATH -> /usr/lib/library-manager/bin/libraries/scala/*<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>/opt/spark/jars/*<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__\\n    SPARK_YARN_STAGING_DIR -> wasbs://6ebbb482-091e-4d4f-8d36-112ba23300db@dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net/user/trusted-service-user/trusted-service-user/.sparkStaging/application_1654782878474_0001\\n    PATH -> /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/usr/local/cuda-11.2/bin:/home/trusted-service-user/cluster-env/env/bin\\n    SPARK_USER -> trusted-service-user\\n    SPARK_HOME -> /opt/spark\\n    PYTHONPATH -> /opt/spark/python/lib/pyspark.zip<CPS>/opt/spark/python/lib/py4j-0.10.7-src.zip<CPS>{{PWD}}/source.zip<CPS>{{PWD}}/setup.zip\\n\\n  command:\\n    LD_LIBRARY_PATH=\\\\\\\"/usr/hdp/current/hadoop-client/lib/native:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/lib/amd64/server:$LD_LIBRARY_PATH\\\\\\\" \\\\ \\n      {{JAVA_HOME}}/bin/java \\\\ \\n      -server \\\\ \\n      -Xmx7168m \\\\ \\n      '-Detwlogger.component=sparkexecutor' \\\\ \\n      '-DlogFilter.filename=SparkLogFilters.xml' \\\\ \\n      '-DpatternGroup.filename=SparkPatternGroups.xml' \\\\ \\n      '-Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer' \\\\ \\n      '-Dlog4jspark.log.dir=/var/log/sparkapp/\\\\${user.name}' \\\\ \\n      '-Dlog4jspark.log.file=sparkexecutor.log' \\\\ \\n      '-Dlog4j.configuration=file:/usr/hdp/current/spark3-client/conf/executor-log4j.properties' \\\\ \\n      '-Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl' \\\\ \\n      '-XX:+UseG1GC' \\\\ \\n      -Djava.io.tmpdir={{PWD}}/tmp \\\\ \\n      '-Dspark.synapse.history.rpc.port=18082' \\\\ \\n      '-Dspark.driver.port=43691' \\\\ \\n      '-Dspark.history.ui.port=18080' \\\\ \\n      '-Dspark.ui.port=0' \\\\ \\n      -Dspark.yarn.app.container.log.dir=<LOG_DIR> \\\\ \\n      -XX:OnOutOfMemoryError='kill %p' \\\\ \\n      org.apache.spark.executor.YarnCoarseGrainedExecutorBackend \\\\ \\n      --driver-url \\\\ \\n      spark://CoarseGrainedScheduler@vm-30847173:43691 \\\\ \\n      --executor-id \\\\ \\n      <executorId> \\\\ \\n      --hostname \\\\ \\n      <hostname> \\\\ \\n      --cores \\\\ \\n      1 \\\\ \\n      --app-id \\\\ \\n      application_1654782878474_0001 \\\\ \\n      --resourceProfileId \\\\ \\n      0 \\\\ \\n      --user-class-path \\\\ \\n      file:$PWD/__app__.jar \\\\ \\n      1><LOG_DIR>/stdout \\\\ \\n      2><LOG_DIR>/stderr\\n\\n  resources:\\n    setup.zip -> resource { scheme: \\\"wasbs\\\" host: \\\"dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net\\\" port: -1 file: \\\"/user/trusted-service-user/trusted-service-user/.sparkStaging/application_1654782878474_0001/setup.zip\\\" userInfo: \\\"6ebbb482-091e-4d4f-8d36-112ba23300db\\\" } size: 75001 timestamp: 1654783064000 type: FILE visibility: PRIVATE\\n    __spark_conf__ -> resource { scheme: \\\"wasbs\\\" host: \\\"dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net\\\" port: -1 file: \\\"/user/trusted-service-user/trusted-service-user/.sparkStaging/application_1654782878474_0001/__spark_conf__.zip\\\" userInfo: \\\"6ebbb482-091e-4d4f-8d36-112ba23300db\\\" } size: 299803 timestamp: 1654783066000 type: ARCHIVE visibility: PRIVATE\\n    source.zip -> resource { scheme: \\\"wasbs\\\" host: \\\"dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net\\\" port: -1 file: \\\"/user/trusted-service-user/trusted-service-user/.sparkStaging/application_1654782878474_0001/source.zip\\\" userInfo: \\\"6ebbb482-091e-4d4f-8d36-112ba23300db\\\" } size: 987 timestamp: 1654783064000 type: FILE visibility: PRIVATE\\n\\n===============================================================================\\n22/06/09 13:58:05 INFO YarnAllocator: Executor Decommissioning Enabled\\n22/06/09 13:58:05 INFO YarnAllocator: Resource profile 0 doesn't exist, adding it\\n22/06/09 13:58:05 INFO Configuration: resource-types.xml not found\\n22/06/09 13:58:05 INFO ResourceUtils: Unable to find 'resource-types.xml'.\\n22/06/09 13:58:05 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@vm-30847173:43691)\\n22/06/09 13:58:05 INFO YarnAllocator: Will request 1 executor container(s) for  ResourceProfile Id: 0, each with 1 core(s) and 7552 MB memory. with custom resources: <memory:7552, vCores:1>\\n22/06/09 13:58:05 INFO YarnAllocator: Submitted 1 unlocalized container requests.\\n22/06/09 13:58:05 INFO ApplicationMaster: Started progress reporter thread with (heartbeat : 1000, initial allocation : 200) intervals\\n22/06/09 13:58:05 INFO DefaultsConfigSparkListener: Persisted __spark_conf_merge_records__.json\\n22/06/09 13:58:05 INFO RpcAppSender: Remote SparkContext server is opened on 10.25.96.7:18083, remoteSparkContext/remoteSparkContextEndpoint\\n22/06/09 13:58:06 INFO YarnAllocator: Launching container container_1654782878474_0001_01_000002 on host vm-30847173 for executor with ID 1 for ResourceProfile Id 0\\n22/06/09 13:58:06 INFO YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.\\n22/06/09 13:58:08 INFO RpcAppSender: Sent driver info of application_1654782878474_0001/1 to RPC history server\\n22/06/09 13:58:08 INFO AsyncEventQueue: Process of event SparkListenerApplicationStart(Azure ML Experiment,Some(application_1654782878474_0001),1654783079436,trusted-service-user,Some(1),Some(Map(stdout -> http://vm-30847173:8042/node/containerlogs/container_1654782878474_0001_01_000001/trusted-service-user/stdout?start=-4096, stderr -> http://vm-30847173:8042/node/containerlogs/container_1654782878474_0001_01_000001/trusted-service-user/stderr?start=-4096)),Some(Map(NM_HTTP_ADDRESS -> vm-30847173:8042, USER -> trusted-service-user, LOG_FILES -> stderr,stdout, NM_HTTP_PORT -> 8042, CLUSTER_ID -> aa0f3207-76d6-4751-9297-f3435744b4b7, NM_PORT -> 35295, HTTP_SCHEME -> http://, NM_HOST -> vm-30847173, CONTAINER_ID -> container_1654782878474_0001_01_000001))) by listener RpcAppListener took 2.988772988s.\\n22/06/09 13:58:10 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.25.96.7:35912) with ID 1,  ResourceProfileId 0\\n22/06/09 13:58:10 INFO YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\\n22/06/09 13:58:10 INFO YarnClusterScheduler: YarnClusterScheduler.postStartHook done\\n22/06/09 13:58:10 INFO SparkContext: Initialized SparkContextAfterInit plugin org.apache.spark.microsoft.tools.api.plugin.MSToolsSparkContextAfterInitPlugin.\\n22/06/09 13:58:11 INFO BlockManagerMasterEndpoint: Registering block manager vm-30847173:40135 with 4.0 GiB RAM, BlockManagerId(1, vm-30847173, 40135, None)\\n22/06/09 13:58:15 INFO SessionTokenBasedTokenProvider: Setting up conf\\n22/06/09 13:58:15 INFO SessionTokenBasedTokenProvider: SessionTokenBasedTokenProvider initialized\\n22/06/09 13:58:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('abfss://synapse@adlsdmpbcpsynwesteu01p.dfs.core.windows.net/synapse/workspaces/synw-dmpbackup-westeu-01p/warehouse') to the value of spark.sql.warehouse.dir ('abfss://synapse@adlsdmpbcpsynwesteu01p.dfs.core.windows.net/synapse/workspaces/synw-dmpbackup-westeu-01p/warehouse').\\n22/06/09 13:58:15 INFO SharedState: Warehouse path is 'abfss://synapse@adlsdmpbcpsynwesteu01p.dfs.core.windows.net/synapse/workspaces/synw-dmpbackup-westeu-01p/warehouse'.\\n22/06/09 13:58:15 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:15 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@58766bb5{/SQL,null,AVAILABLE,@Spark}\\n22/06/09 13:58:15 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:15 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@218e8d0d{/SQL/json,null,AVAILABLE,@Spark}\\n22/06/09 13:58:15 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:15 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@c6b393{/SQL/execution,null,AVAILABLE,@Spark}\\n22/06/09 13:58:15 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:15 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2c06b4e8{/SQL/execution/json,null,AVAILABLE,@Spark}\\n22/06/09 13:58:15 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\\n22/06/09 13:58:15 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@19e4fd44{/static/sql,null,AVAILABLE,@Spark}\\nANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.822/06/09 13:58:18 INFO SharedState: Creating database default if not exists\\n22/06/09 13:58:18 INFO HiveConf: Found configuration file file:/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/filecache/11/__spark_conf__.zip/__hadoop_conf__/hive-site.xml\\n22/06/09 13:58:18 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3 using file:/opt/hive-metastore/lib-2.3/hive-metastore-2.3.2.2.6.99.201-SNAPSHOT.jar:file:/opt/hive-metastore/lib-2.3/javax.jdo-3.2.0-m3.jar:file:/opt/hive-metastore/lib-2.3/hive-common-2.3.2.2.6.99.201-SNAPSHOT.jar:file:/opt/hive-metastore/lib-2.3/microsoft-catalog-metastore-client-1.0.59.jar:file:/opt/hive-metastore/lib-2.3/datanucleus-rdbms-4.1.19.jar:file:/opt/hive-metastore/lib-2.3/libfb303-0.9.3.jar:file:/opt/hive-metastore/lib-2.3/servlet-api-2.4.jar:file:/opt/hive-metastore/lib-2.3/mysql-connector-java-8.0.18.jar:file:/opt/hive-metastore/lib-2.3/commons-logging-1.2.jar:file:/opt/hive-metastore/lib-2.3/hive-exec-2.3.2.2.6.99.201-SNAPSHOT.jar:file:/opt/hive-metastore/lib-2.3/datanucleus-core-4.1.17.jar:file:/opt/hive-metastore/lib-2.3/datanucleus-api-jdo-4.2.4.jar:file:/opt/hive-metastore/lib-2.3/bonecp-0.8.0.RELEASE.jar:file:/opt/hive-metastore/lib-2.3/antlr-runtime-3.5.2.jar:file:/usr/hdp/current/hadoop-client/lib/woodstox-core-5.0.3.jar:file:/usr/hdp/current/hadoop-client/lib/kerby-asn1-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/gson-2.2.4.jar:file:/usr/hdp/current/hadoop-client/lib/stax2-api-3.1.4.jar:file:/usr/hdp/current/hadoop-client/lib/kerby-xdr-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/commons-cli-1.2.jar:file:/usr/hdp/current/hadoop-client/lib/commons-codec-1.11.jar:file:/usr/hdp/current/hadoop-client/lib/netty-3.10.5.Final.jar:file:/usr/hdp/current/hadoop-client/lib/json-smart-2.3.jar:file:/usr/hdp/current/hadoop-client/lib/jackson-jaxrs-1.9.13.jar:file:/usr/hdp/current/hadoop-client/lib/jackson-mapper-asl-1.9.13.jar:file:/usr/hdp/current/hadoop-client/lib/jul-to-slf4j-1.7.25.jar:file:/usr/hdp/current/hadoop-client/lib/json4s-scalap_2.12-3.7.0-M5.jar:file:/usr/hdp/current/hadoop-client/lib/jersey-core-1.19.jar:file:/usr/hdp/current/hadoop-client/lib/accessors-smart-1.2.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-server-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/commons-collections-3.2.2.jar:file:/usr/hdp/current/hadoop-client/lib/jackson-databind-2.10.0.jar:file:/usr/hdp/current/hadoop-client/lib/json4s-jackson_2.12-3.7.0-M5.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-simplekdc-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/jsch-0.1.54.jar:file:/usr/hdp/current/hadoop-client/lib/flogger-system-backend-0.3.1.jar:file:/usr/hdp/current/hadoop-client/lib/lz4-1.2.0.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-server-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/slf4j-log4j12-1.7.25.jar:file:/usr/hdp/current/hadoop-client/lib/metrics-core-3.2.4.jar:file:/usr/hdp/current/hadoop-client/lib/aliyun-sdk-oss-2.8.3.jar:file:/usr/hdp/current/hadoop-client/lib/kerby-config-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/commons-lang-2.6.jar:file:/usr/hdp/current/hadoop-client/lib/commons-compress-1.4.1.jar:file:/usr/hdp/current/hadoop-client/lib/token-provider-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/curator-framework-2.12.0.jar:file:/usr/hdp/current/hadoop-client/lib/jersey-json-1.19.jar:file:/usr/hdp/current/hadoop-client/lib/commons-net-3.6.jar:file:/usr/hdp/current/hadoop-client/lib/kerby-util-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/json4s-ast_2.12-3.7.0-M5.jar:file:/usr/hdp/current/hadoop-client/lib/commons-configuration2-2.1.1.jar:file:/usr/hdp/current/hadoop-client/lib/guava-28.0-jre.jar:file:/usr/hdp/current/hadoop-client/lib/azure-storage-7.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/azure-data-lake-store-sdk-2.3.9.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-util-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/httpclient-4.5.2.jar:file:/usr/hdp/current/hadoop-client/lib/commons-beanutils-1.9.4.jar:file:/usr/hdp/current/hadoop-client/lib/commons-logging-1.1.3.jar:file:/usr/hdp/current/hadoop-client/lib/gcs-connector-hadoop3-1.9.10-shaded.jar:file:/usr/hdp/current/hadoop-client/lib/jaxb-api-2.2.11.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-webapp-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/jsp-api-2.1.jar:file:/usr/hdp/current/hadoop-client/lib/commons-lang3-3.4.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-identity-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/jersey-server-1.19.jar:file:/usr/hdp/current/hadoop-client/lib/commons-math3-3.1.1.jar:file:/usr/hdp/current/hadoop-client/lib/jackson-core-asl-1.9.13.jar:file:/usr/hdp/current/hadoop-client/lib/javax.servlet-api-3.1.0.jar:file:/usr/hdp/current/hadoop-client/lib/zookeeper-3.4.6.5.0-62565507.jar:file:/usr/hdp/current/hadoop-client/lib/nimbus-jose-jwt-4.41.1.jar:file:/usr/hdp/current/hadoop-client/lib/snappy-java-1.0.5.jar:file:/usr/hdp/current/hadoop-client/lib/xz-1.0.jar:file:/usr/hdp/current/hadoop-client/lib/curator-recipes-2.12.0.jar:file:/usr/hdp/current/hadoop-client/lib/log4j-1.2.17.jar:file:/usr/hdp/current/hadoop-client/lib/curator-client-2.12.0.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-client-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/azure-keyvault-core-1.0.0.jar:file:/usr/hdp/current/hadoop-client/lib/jackson-xc-1.9.13.jar:file:/usr/hdp/current/hadoop-client/lib/jdom-1.1.jar:file:/usr/hdp/current/hadoop-client/lib/httpcore-4.4.4.jar:file:/usr/hdp/current/hadoop-client/lib/TokenLibrary-assembly-3.2.0.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-io-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/jackson-core-2.10.0.jar:file:/usr/hdp/current/hadoop-client/lib/jaxb-impl-2.2.3-1.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-core-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/slf4j-api-1.7.25.jar:file:/usr/hdp/current/hadoop-client/lib/kerby-pkix-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/json4s-core_2.12-3.7.0-M5.jar:file:/usr/hdp/current/hadoop-client/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:file:/usr/hdp/current/hadoop-client/lib/re2j-1.1.jar:file:/usr/hdp/current/hadoop-client/lib/htrace-core4-4.1.0-incubating.jar:file:/usr/hdp/current/hadoop-client/lib/wildfly-openssl-1.0.7.Final.jar:file:/usr/hdp/current/hadoop-client/lib/aws-java-sdk-bundle-1.11.375.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-admin-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/protobuf-java-2.5.0.jar:file:/usr/hdp/current/hadoop-client/lib/asm-5.0.4.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-security-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/animal-sniffer-annotations-1.17.jar:file:/usr/hdp/current/hadoop-client/lib/j2objc-annotations-1.3.jar:file:/usr/hdp/current/hadoop-client/lib/failureaccess-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/ojalgo-43.0.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-crypto-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-util-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-http-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-xml-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/jcip-annotations-1.0-1.jar:file:/usr/hdp/current/hadoop-client/lib/checker-qual-2.8.1.jar:file:/usr/hdp/current/hadoop-client/lib/flogger-0.3.1.jar:file:/usr/hdp/current/hadoop-client/lib/kerb-common-1.0.1.jar:file:/usr/hdp/current/hadoop-client/lib/jackson-annotations-2.10.0.jar:file:/usr/hdp/current/hadoop-client/lib/jettison-1.1.jar:file:/usr/hdp/current/hadoop-client/lib/kafka-clients-0.8.2.1.jar:file:/usr/hdp/current/hadoop-client/lib/jersey-servlet-1.19.jar:file:/usr/hdp/current/hadoop-client/lib/jetty-servlet-9.3.24.v20180605.jar:file:/usr/hdp/current/hadoop-client/lib/avro-1.7.7.jar:file:/usr/hdp/current/hadoop-client/lib/error_prone_annotations-2.3.2.jar:file:/usr/hdp/current/hadoop-client/lib/google-extensions-0.3.1.jar:file:/usr/hdp/current/hadoop-client/lib/commons-io-2.5.jar:file:/usr/hdp/current/hadoop-client/lib/flogger-log4j-backend-0.3.1.jar:file:/usr/hdp/current/hadoop-client/lib/jsr311-api-1.1.1.jar:file:/usr/hdp/current/hadoop-client/lib/jsr305-3.0.0.jar\\n22/06/09 13:58:18 INFO HiveConf: Found configuration file null\\n22/06/09 13:58:19 INFO SessionState: Created HDFS directory: wasbs://6ebbb482-091e-4d4f-8d36-112ba23300db@dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net/tmp/hive/trusted-service-user\\n22/06/09 13:58:19 INFO SessionState: Created local directory: /mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1654782878474_0001/container_1654782878474_0001_01_000001/tmp/trusted-service-user\\n22/06/09 13:58:20 INFO SessionState: Created HDFS directory: wasbs://6ebbb482-091e-4d4f-8d36-112ba23300db@dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net/tmp/hive/trusted-service-user/7d976e24-57d2-428d-97c2-02606e017e56\\n22/06/09 13:58:20 INFO SessionState: Created local directory: /mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1654782878474_0001/container_1654782878474_0001_01_000001/tmp/trusted-service-user/7d976e24-57d2-428d-97c2-02606e017e56\\n22/06/09 13:58:20 INFO SessionState: Created HDFS directory: wasbs://6ebbb482-091e-4d4f-8d36-112ba23300db@dqbv8du8i2usk4481m5tpjr7.blob.core.windows.net/tmp/hive/trusted-service-user/7d976e24-57d2-428d-97c2-02606e017e56/_tmp_space.db\\n22/06/09 13:58:20 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is abfss://synapse@adlsdmpbcpsynwesteu01p.dfs.core.windows.net/synapse/workspaces/synw-dmpbackup-westeu-01p/warehouse\\n22/06/09 13:58:20 INFO HiveMetastoreClientFactory: Checking hive.synapse.externalmetastore.linkedservice.name in Hive configuration = ls_mysql_hms\\n22/06/09 13:58:20 INFO MetastoreConfigurationProvider: getLinkedServiceName = ls_mysql_hms\\n22/06/09 13:58:21 WARN DefaultTimer: Can not service-load a timer. Using JavaTimer instead.\\n22/06/09 13:58:21 INFO TokenLibraryLinkedService: Attempting to fetch access token from cache\\n22/06/09 13:58:21 WARN InMemoryCacheClient: Token not found in in-memory cache\\n22/06/09 13:58:21 INFO TokenLibraryLinkedService: Invoking token service to fetch access token\\n22/06/09 13:58:21 INFO TokenLibraryLinkedService: Fetching access token\\n22/06/09 13:58:22 INFO finagle: Finagle version 19.5.1 (rev=21b8a8b3eeca571eedc7094df53d5eb806856b61) built at 20190520-194112\\n22/06/09 13:58:29 ERROR TokenLibraryLinkedService: POST failed {\\\"result\\\":\\\"DependencyError\\\",\\\"errorId\\\":\\\"InternalServerError\\\",\\\"errorMessage\\\":\\\"LSRServiceException is [{\\\\\\\"StatusCode\\\\\\\":401,\\\\\\\"ErrorResponse\\\\\\\":{\\\\\\\"code\\\\\\\":\\\\\\\"AccessControlUnauthorized\\\\\\\",\\\\\\\"message\\\\\\\":\\\\\\\"Insufficient permissions to call this API. 4920cd8e-47e7-4407-8d4b-2dc91a133c2f does not have Microsoft.Synapse/workspaces/read, Microsoft.Synapse/workspaces/linkedServices/useSecret/action on scope workspaces/synw-dmpbackup-westeu-01p/linkedServices/ls_mysql_hms\\\\\\\",\\\\\\\"target\\\\\\\":null},\\\\\\\"StackTrace\\\\\\\":\\\\\\\"   at Microsoft.Marlin.Common.ADF.Impl.LSRClient.CheckForFailures(HttpResponseMessage response, String responseContent) in C:\\\\\\\\\\\\\\\\source\\\\\\\\\\\\\\\\Common\\\\\\\\\\\\\\\\Microsoft.Marlin.Common.ADF\\\\\\\\\\\\\\\\Impl\\\\\\\\\\\\\\\\LSRClient.cs:line 348\\\\\\\\r\\\\\\\\n   at Microsoft.Marlin.Common.ADF.Impl.LSRClient.SendAsync(HttpRequestMessage request, CancellationToken cancellationToken, String traceId) in C:\\\\\\\\\\\\\\\\source\\\\\\\\\\\\\\\\Common\\\\\\\\\\\\\\\\Microsoft.Marlin.Common.ADF\\\\\\\\\\\\\\\\Impl\\\\\\\\\\\\\\\\LSRClient.cs:line 365\\\\\\\\r\\\\\\\\n   at Microsoft.Marlin.Common.ADF.Impl.LSRClient.ResolveLinkedServiceAsync(String linkedServiceName, ResolveAudienceRequest request, String traceId, CancellationToken cancellationToken) in C:\\\\\\\\\\\\\\\\source\\\\\\\\\\\\\\\\Common\\\\\\\\\\\\\\\\Microsoft.Marlin.Common.ADF\\\\\\\\\\\\\\\\Impl\\\\\\\\\\\\\\\\LSRClient.cs:line 202\\\\\\\\r\\\\\\\\n   at Microsoft.Marlin.TokenService.Token.LSRAudienceTokenProvider.GetToken(Boolean isLinkedService, String audience, String sessionToken, CancellationToken cancellationToken) in C:\\\\\\\\\\\\\\\\source\\\\\\\\\\\\\\\\TokenService\\\\\\\\\\\\\\\\Microsoft.Marlin.TokenService\\\\\\\\\\\\\\\\Token\\\\\\\\\\\\\\\\LSRAudienceTokenProvider.cs:line 153\\\\\\\\r\\\\\\\\n   at Microsoft.Marlin.TokenService.Token.LSRAudienceTokenProvider.GetTokenForAudienceAsync(Boolean isLinkedService, String audience, String account, String sessionToken, SignaturePayload signaturePayload, CancellationToken cancellationToken) in C:\\\\\\\\\\\\\\\\source\\\\\\\\\\\\\\\\TokenService\\\\\\\\\\\\\\\\Microsoft.Marlin.TokenService\\\\\\\\\\\\\\\\Token\\\\\\\\\\\\\\\\LSRAudienceTokenProvider.cs:line 127\\\\\\\\r\\\\\\\\n   at Microsoft.Marlin.TokenService.Controllers.TokenController.GetTokenAsync(TokenRequest request, CancellationToken cancellationToken) in C:\\\\\\\\\\\\\\\\source\\\\\\\\\\\\\\\\TokenService\\\\\\\\\\\\\\\\Microsoft.Marlin.TokenService\\\\\\\\\\\\\\\\Controllers\\\\\\\\\\\\\\\\TokenController.cs:line 82\\\\\\\\r\\\\\\\\n   at lambda_method535(Closure , Object )\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ActionMethodExecutor.AwaitableObjectResultExecutor.Execute(IActionResultTypeMapper mapper, ObjectMethodExecutor executor, Object controller, Object[] arguments)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ControllerActionInvoker.<InvokeActionMethodAsync>g__Awaited|12_0(ControllerActionInvoker invoker, ValueTask`1 actionResultValueTask)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ControllerActionInvoker.<InvokeNextActionFilterAsync>g__Awaited|10_0(ControllerActionInvoker invoker, Task lastTask, State next, Scope scope, Object state, Boolean isCompleted)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ControllerActionInvoker.Rethrow(ActionExecutedContextSealed context)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ControllerActionInvoker.Next(State& next, Scope& scope, Object& state, Boolean& isCompleted)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ControllerActionInvoker.<InvokeInnerFilterAsync>g__Awaited|13_0(ControllerActionInvoker invoker, Task lastTask, State next, Scope scope, Object state, Boolean isCompleted)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ResourceInvoker.<InvokeNextResourceFilter>g__Awaited|24_0(ResourceInvoker invoker, Task lastTask, State next, Scope scope, Object state, Boolean isCompleted)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ResourceInvoker.Rethrow(ResourceExecutedContextSealed context)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ResourceInvoker.Next(State& next, Scope& scope, Object& state, Boolean& isCompleted)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ResourceInvoker.<InvokeFilterPipelineAsync>g__Awaited|19_0(ResourceInvoker invoker, Task lastTask, State next, Scope scope, Object state, Boolean isCompleted)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Mvc.Infrastructure.ResourceInvoker.<InvokeAsync>g__Awaited|17_0(ResourceInvoker invoker, Task task, IDisposable scope)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Routing.EndpointMiddleware.<Invoke>g__AwaitRequestTask|6_0(Endpoint endpoint, Task requestTask, ILogger logger)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Authorization.AuthorizationMiddleware.Invoke(HttpContext context)\\\\\\\\r\\\\\\\\n   at Swashbuckle.AspNetCore.SwaggerUI.SwaggerUIMiddleware.Invoke(HttpContext httpContext)\\\\\\\\r\\\\\\\\n   at Swashbuckle.AspNetCore.Swagger.SwaggerMiddleware.Invoke(HttpContext httpContext, ISwaggerProvider swaggerProvider)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Builder.Extensions.MapWhenMiddleware.Invoke(HttpContext context)\\\\\\\\r\\\\\\\\n   at Microsoft.AspNetCore.Diagnostics.ExceptionHandlerMiddleware.<Invoke>g__Awaited|6_0(ExceptionHandlerMiddleware middleware, HttpContext context, Task task)\\\\\\\",\\\\\\\"Message\\\\\\\":\\\\\\\"Insufficient permissions to call this API. 4920cd8e-47e7-4407-8d4b-2dc91a133c2f does not have Microsoft.Synapse/workspaces/read, Microsoft.Synapse/workspaces/linkedServices/useSecret/action on scope workspaces/synw-dmpbackup-westeu-01p/linkedServices/ls_mysql_hms\\\\\\\",\\\\\\\"Data\\\\\\\":{},\\\\\\\"InnerException\\\\\\\":null,\\\\\\\"HelpLink\\\\\\\":null,\\\\\\\"Source\\\\\\\":\\\\\\\"Microsoft.Marlin.Common.ADF\\\\\\\",\\\\\\\"HResult\\\\\\\":-2146233088}]. TraceId : f6d1505d-0456-4de0-8476-7a74000ac165. Error Component : LSR\\\"}\\n22/06/09 13:58:34 INFO finagle: FailureAccrualFactory marking connection to \\\"tokenservice1.westeurope.azuresynapse.net:443\\\" as dead. Remote Address: Inet(tokenservice1.westeurope.azuresynapse.net/10.250.0.7:443,Map())\\n22/06/09 13:58:42 INFO finagle: FailureAccrualFactory marking connection to \\\"tokenservice1.westeurope.azuresynapse.net:443\\\" as dead. Remote Address: Inet(tokenservice1.westeurope.azuresynapse.net/10.250.0.7:443,Map())\\n22/06/09 13:58:51 INFO finagle: FailureAccrualFactory marking connection to \\\"tokenservice1.westeurope.azuresynapse.net:443\\\" as dead. Remote Address: Inet(tokenservice1.westeurope.azuresynapse.net/10.250.0.7:443,Map())\\n22/06/09 13:59:23 ERROR TokenLibraryLinkedService: POST failed\\ncom.twitter.util.TimeoutException: 1.minutes\\n\\tat com.twitter.util.Future.$anonfun$within$1(Future.scala:1642)\\n\\tat com.twitter.util.Future$$anon$4.apply$mcV$sp(Future.scala:1693)\\n\\tat com.twitter.util.Monitor.apply(Monitor.scala:46)\\n\\tat com.twitter.util.Monitor.apply$(Monitor.scala:41)\\n\\tat com.twitter.util.NullMonitor$.apply(Monitor.scala:229)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$2(Timer.scala:39)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.twitter.util.Local$.let(Local.scala:4904)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$1(Timer.scala:39)\\n\\tat com.twitter.util.Monitor.apply(Monitor.scala:46)\\n\\tat com.twitter.util.Monitor.apply$(Monitor.scala:41)\\n\\tat com.twitter.util.NullMonitor$.apply(Monitor.scala:229)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$2(Timer.scala:39)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.twitter.util.Local$.let(Local.scala:4904)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$1(Timer.scala:39)\\n\\tat com.twitter.util.JavaTimer.$anonfun$scheduleOnce$1(Timer.scala:233)\\n\\tat com.twitter.util.JavaTimer$$anon$3.run(Timer.scala:264)\\n\\tat java.util.TimerThread.mainLoop(Timer.java:555)\\n\\tat java.util.TimerThread.run(Timer.java:505)\\n22/06/09 13:59:23 ERROR MetastoreConfigurationProvider: Failed to get properties map of linked service [ls_mysql_hms] via TokenLibrary. Exception = {}\\ncom.twitter.util.TimeoutException: 1.minutes\\n\\tat com.twitter.util.Future.$anonfun$within$1(Future.scala:1642)\\n\\tat com.twitter.util.Future$$anon$4.apply$mcV$sp(Future.scala:1693)\\n\\tat com.twitter.util.Monitor.apply(Monitor.scala:46)\\n\\tat com.twitter.util.Monitor.apply$(Monitor.scala:41)\\n\\tat com.twitter.util.NullMonitor$.apply(Monitor.scala:229)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$2(Timer.scala:39)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.twitter.util.Local$.let(Local.scala:4904)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$1(Timer.scala:39)\\n\\tat com.twitter.util.Monitor.apply(Monitor.scala:46)\\n\\tat com.twitter.util.Monitor.apply$(Monitor.scala:41)\\n\\tat com.twitter.util.NullMonitor$.apply(Monitor.scala:229)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$2(Timer.scala:39)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.twitter.util.Local$.let(Local.scala:4904)\\n\\tat com.twitter.util.Timer.$anonfun$schedule$1(Timer.scala:39)\\n\\tat com.twitter.util.JavaTimer.$anonfun$scheduleOnce$1(Timer.scala:233)\\n\\tat com.twitter.util.JavaTimer$$anon$3.run(Timer.scala:264)\\n\\tat java.util.TimerThread.mainLoop(Timer.java:555)\\n\\tat java.util.TimerThread.run(Timer.java:505)\\n22/06/09 13:59:23 WARN Hive: Failed to register all functions.\\norg.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Failed to get metastore properties from the linked service.)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3901)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:250)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:233)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:391)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:334)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:314)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:290)\\n\\tat org.apache.spark.sql.hive.client.HiveClientImpl.client(HiveClientImpl.scala:257)\\n\\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:283)\\n\\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:224)\\n\\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:223)\\n\\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:273)\\n\\tat org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:336)\\n\\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createDatabase$1(HiveExternalCatalog.scala:193)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\\n\\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:193)\\n\\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:137)\\n\\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:124)\\n\\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:153)\\n\\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:151)\\n\\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$2(HiveSessionStateBuilder.scala:60)\\n\\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:99)\\n\\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:99)\\n\\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupGlobalTempView(SessionCatalog.scala:870)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveTempViews$$lookupTempView(Analyzer.scala:916)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveTempViews$$lookupAndResolveTempView(Analyzer.scala:930)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$$anonfun$apply$7.applyOrElse(Analyzer.scala:875)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$$anonfun$apply$7.applyOrElse(Analyzer.scala:873)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveTempViews$.apply(Analyzer.scala:873)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1112)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1077)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\\n\\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\\n\\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\\n\\tat scala.collection.immutable.List.foldLeft(List.scala:89)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\\n\\tat scala.collection.immutable.List.foreach(List.scala:392)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:93)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:75)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:120)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:159)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:159)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:75)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:73)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:65)\\n\\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\\n\\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\tat java.lang.Thread.run(Thread.java:748)\\nCaused by: MetaException(message:Failed to get metastore properties from the linked service.)\\n\\tat com.microsoft.catalog.metastore.externalprovider.LinkedServiceMetastoreProvider.validateLinkedService(LinkedServiceMetastoreProvider.java:40)\\n\\tat com.microsoft.catalog.metastore.externalprovider.LinkedServiceMetastoreProvider.getAzureDatabaseLinkedService(LinkedServiceMetastoreProvider.java:104)\\n\\tat com.microsoft.catalog.metastore.externalprovider.LinkedServiceMetastoreProvider.init(LinkedServiceMetastoreProvider.java:34)\\n\\tat com.microsoft.catalog.metastore.externalprovider.MetastoreConfigurationProvider.applyTo(MetastoreConfigurationProvider.java:53)\\n\\tat com.microsoft.catalog.metastore.metastoreclient.HiveMetastoreClientFactory.createExternalHivemetasoreClient(HiveMetastoreClientFactory.java:85)\\n\\tat com.microsoft.catalog.metastore.metastoreclient.HiveMetastoreClientFactory.createMetaStoreClient(HiveMetastoreClientFactory.java:65)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3606)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3656)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3636)\\n\\tat org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3898)\\n\\t... 120 more\\n\\nError occurred: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Failed to get metastore properties from the linked service.)\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.42.0\"}, \"loading\": false}"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RunId: synapse-spark_1654782772_9f61aa5e\n",
            "Web View: https://ml.azure.com/runs/synapse-spark_1654782772_9f61aa5e?wsid=/subscriptions/25a89471-60b3-4c91-b44f-ca49f38e6137/resourcegroups/rg-dmpbackup-westeu-01p/workspaces/mlw-dmpbackup-westeu-01p&tid=041d21aa-b4ab-4ad1-891d-62207b3367ef\n"
          ]
        }
      ],
      "source": [
        "from azureml.core import Experiment\n",
        "\n",
        "exp = Experiment(workspace=ws, name=\"synapse-spark\") \n",
        "run = exp.submit(config=script_run_config) \n",
        "RunDetails(run).show()\n",
        "run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
